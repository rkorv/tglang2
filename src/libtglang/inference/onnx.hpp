#pragma once

#include <vector>

#include <onnxruntime/core/session/onnxruntime_cxx_api.h>

#include <ort_model.hpp>  // Generated by `xxd -i model.ort > model.h`
#include <model_meta.hpp> // Generated by python script

#include "../tglang.h"

class TglangModelInference {
private:
    Ort::Env env;
    Ort::Session session;
    Ort::MemoryInfo memory_info;


public:
    TglangModelInference() : env(ORT_LOGGING_LEVEL_WARNING, "Inference"),
                             session(env, reinterpret_cast<const void*>(onnx_model_ort), onnx_model_ort_len, Ort::SessionOptions{nullptr}),
                             memory_info(Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault)) {}


    enum TglangLanguage forward(const std::vector<det_int_t>& encoded_text,
            const std::vector<det_int_t>& naming_types, const std::vector<det_int_t>& positions_ids)
    {
        size_t size_naming_types = naming_types.size() * sizeof(det_int_t);
        size_t size_positions_ids = positions_ids.size() * sizeof(det_int_t);
        size_t size_encoded_text = encoded_text.size() * sizeof(det_int_t);

        std::vector<int64_t> input_tensor_shape = {1, static_cast<int64_t>(encoded_text.size())};

        Ort::Value input_tensor0 = Ort::Value::CreateTensor(memory_info, const_cast<det_int_t*>(naming_types.data()), size_naming_types, input_tensor_shape.data(), input_tensor_shape.size());
        Ort::Value input_tensor1 = Ort::Value::CreateTensor(memory_info, const_cast<det_int_t*>(positions_ids.data()), size_positions_ids, input_tensor_shape.data(), input_tensor_shape.size());
        Ort::Value input_tensor2 = Ort::Value::CreateTensor(memory_info, const_cast<det_int_t*>(encoded_text.data()), size_encoded_text, input_tensor_shape.data(), input_tensor_shape.size());

        const char* input_node_names[] = {"naming_types", "positions", "encoded_text"};
        const char* output_node_names[] = {"label", "label_conf", "is_code", "is_code_conf"};

        std::array<Ort::Value, 3> input_tensors = {std::move(input_tensor0), std::move(input_tensor1), std::move(input_tensor2)};
        auto output_tensors = session.Run(Ort::RunOptions{nullptr}, input_node_names, input_tensors.data(), 3, output_node_names, 4);

        float is_code_conf = *output_tensors[3].GetTensorData<float>();
        if (is_code_conf < IS_CODE_THRESHOLD)
            return TGLANG_LANGUAGE_OTHER;

        float label_conf = *output_tensors[1].GetTensorData<float>();
        if (label_conf < DETECTION_THRESHOLD)
            return TGLANG_LANGUAGE_OTHER;

        det_int_t label = *output_tensors[0].GetTensorData<det_int_t>();

        return static_cast<TglangLanguage>(label);
    }

};
